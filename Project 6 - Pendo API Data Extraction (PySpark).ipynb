{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e00762e-f0e4-40c9-b4d6-dae9910eea13",
     "showTitle": true,
     "title": "Pendo data functions"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import http.client\n",
    "import json\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "from pyspark.sql.functions import from_json\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "storage_account_key = dbutils.secrets.get(\"fakeprod\", \"fakekey\")\n",
    "analytics_pendo_apikey = dbutils.secrets.get(\"fakeprod\", \"fakekey\") \n",
    "pendo_key=analytics_pendo_apikey\n",
    "spark = SparkSession.builder.appName(\"Dynamic Nested JSON Flatten Versions\").getOrCreate()\n",
    "\n",
    "\n",
    "def get_pendo_RawData(object_name,APIlist = []):\n",
    "  \"\"\"  \n",
    "  purpose:\n",
    "    This function gets all the specified pendo api collections into ADLS blob storage.\n",
    "  parameters:\n",
    "    object_name to save the data under , API urls as list \n",
    "  \"\"\"\n",
    "  status = ''\n",
    "  execution_log = ''\n",
    "  \n",
    "  \n",
    "  try:\n",
    "    conn = http.client.HTTPSConnection(\"app.pendo.io\")\n",
    "    payload = ''\n",
    "    headers = {\n",
    "      'Content-Type': 'application/json',\n",
    "      'x-pendo-integration-key': pendo_key\n",
    "    } \n",
    "\n",
    "    for index, value in enumerate(APIlist):      \n",
    "      conn.request(\"GET\", f\"{value}\",payload,headers=headers)\n",
    "      response = conn.getresponse() \n",
    "      raw_data = response.read().decode('utf-8')\n",
    "      json_data = json.loads(raw_data)\n",
    "      schema = StructType([StructField(\"json_string\", StringType(), True)])      \n",
    "\n",
    "      # print(json_data)\n",
    "      # data_type = type(json_data).__name__\n",
    "      # print(data_type)       \n",
    "      \n",
    "      # Create a Spark DataFrame from the data, converting each item to a JSON string.\n",
    "      if isinstance(json_data,list):\n",
    "        df = spark.createDataFrame([(json.dumps(item),) for item in json_data], schema) \n",
    "      elif isinstance(json_data,dict):        \n",
    "        json_data_str = json.dumps([json_data])        \n",
    "        df = spark.createDataFrame([(json_data_str,)],schema)\n",
    "\n",
    "      spark.conf.set(\"storage_acocunt_url\", storage_account_key)\n",
    "      df.write.format(\"delta\").mode(\"append\").option(\"mergeSchema\",\"true\").save(\"storage_account_container_url\"+f\"{object_name}\"+\"/\")  \n",
    "      \n",
    "      # df.write.format(\"delta\").mode(\"append\").option(\"mergeSchema\",\"true\").saveAsTable(\"pendo_accounts_raw__test\")  \n",
    "      conn.close()\n",
    "    \n",
    "    \n",
    "    status = 'success'\n",
    "    execution_log += f\" No Errors\"\n",
    "    print(\"Completed loading \"+f\"{object_name}\"+ \" to bronze folder\")\n",
    "\n",
    "\n",
    "  except Exception as execution_error:\n",
    "    status = 'failed'\n",
    "    execution_log += f\"Error: {str(execution_error)}\"\n",
    "\n",
    "  return status, execution_log\n",
    "  \n",
    "def get_pendo_RawDataAggregation(object_name,payloads,DataWriteMode, APIlist = []):\n",
    "  \"\"\"  \n",
    "  purpose:\n",
    "    This function gets all the specified pendo api collections into ADLS blob storage.\n",
    "  parameters:\n",
    "    object_name to save the data under , API urls as list \n",
    "  \"\"\"\n",
    "  status = ''\n",
    "  execution_log = ''\n",
    "  \n",
    "  \n",
    "  try:\n",
    "    conn = http.client.HTTPSConnection(\"app.pendo.io\")    \n",
    "    headers = {\n",
    "      'Content-Type': 'application/json',\n",
    "      'x-pendo-integration-key': pendo_key\n",
    "    } \n",
    "\n",
    "    for index, value in enumerate(APIlist):      \n",
    "      conn.request(\"POST\", f\"{value}\", payloads,headers)\n",
    "      response = conn.getresponse() \n",
    "      raw_data = response.read().decode('utf-8')\n",
    "      json_data = json.loads(raw_data)\n",
    "      json_data = json_data.get(\"results\",[])\n",
    "      schema = StructType([StructField(\"json_string\", StringType(), True)])      \n",
    "\n",
    "      # print(json_data)\n",
    "      # data_type = type(json_data).__name__\n",
    "      # print(data_type)       \n",
    "      \n",
    "      # Create a Spark DataFrame from the data, converting each item to a JSON string.\n",
    "      if isinstance(json_data,list):\n",
    "        df = spark.createDataFrame([(json.dumps(item),) for item in json_data], schema) \n",
    "      elif isinstance(json_data,dict):        \n",
    "        json_data_str = json.dumps([json_data])        \n",
    "        df = spark.createDataFrame([(json_data_str,)],schema)\n",
    "\n",
    "      # Correctly specifying the catalog and database\n",
    "      spark.sql(\"USE CATALOG silver_prod_fr\")\n",
    "\n",
    "      spark.conf.set(\"storage_acocunt_url\", storage_account_key)\n",
    "      df.write.format(\"delta\").mode(f\"{DataWriteMode}\").option(\"mergeSchema\",\"true\").save(\"storage_account_container_url\"+f\"{object_name}\"+\"/\")\n",
    "\n",
    "      # df.show()\n",
    "      # df.write.format(\"delta\").mode(\"append\").option(\"mergeSchema\",\"true\").saveAsTable(\"pendoTEST_\"+f\"{object_name}\")  \n",
    "      conn.close()\n",
    "\n",
    "    \n",
    "    status = 'success'\n",
    "    execution_log += f\" No Errors\"\n",
    "    print(\"Completed loading \"+f\"{object_name}\"+ \" to bronze folder\")\n",
    "\n",
    "\n",
    "  except Exception as execution_error:\n",
    "    status = 'failed'\n",
    "    execution_log += f\"Error: {str(execution_error)}\"\n",
    "\n",
    "  return status, execution_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13beec5d-cf66-49c9-8187-0df3d94661d3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#Provide the start date and end date before the run\n",
    "starttime = datetime.datetime(2023,1,31)\n",
    "delta = datetime.timedelta(days=10)\n",
    "endtime = starttime+delta\n",
    "\n",
    "enddate = datetime.datetime(2024,4,22)\n",
    "\n",
    "Data_WriteMode = \"append\" # overwrite\n",
    "\n",
    "while(endtime<=enddate):\n",
    "  AggregationData_StartDate = str(int(starttime.timestamp()*1000)+10000)\n",
    "  AggregationData_EndDate = str(int(endtime.timestamp()*1000))\n",
    "  print(\"StartTime: \"+ f\"{starttime}\" + \"  StartTime_epoch: \"+ AggregationData_StartDate)\n",
    "  print(\"EndTime:   \"+ f\"{endtime}\" + \"  EndTime_epoch:   \"+ AggregationData_EndDate)\n",
    "  print(\"######################################################\")\n",
    "  starttime+=delta\n",
    "  endtime+=delta  \n",
    "\n",
    " \n",
    "  conn = http.client.HTTPSConnection(\"app.pendo.io\")\n",
    "  payload = ''\n",
    "  headers = {\n",
    "    'Content-Type': 'application/json',\n",
    "    'x-pendo-integration-key': pendo_key\n",
    "  }\n",
    "\n",
    "\n",
    "  # \"\"\"\n",
    "  #  Getting the list of all Account IDs using a report.\n",
    "  # \"\"\"\n",
    "  # conn.request(\"GET\", \"/api/v1/report/accounts_report_url/results.json\", payload, headers)\n",
    "  # accounts_response = conn.getresponse().read().decode(\"utf-8\")\n",
    "  # accounts_json = json.loads(accounts_response)\n",
    "  # GETAccountsList = []\n",
    "  # for item in accounts_json:\n",
    "  #   account_id = item['accountId']\n",
    "  #   GETAccountsList.append(\"/api/v1/account/\"+account_id)\n",
    "  # conn.close()\n",
    "\n",
    "\n",
    "\n",
    "  # \"\"\"\n",
    "  #  Getting the list of all Visitor IDs using a report.\n",
    "  # \"\"\"\n",
    "  # conn.request(\"GET\", \"/api/v1/report/visitors_report_url/results.json\", payload, headers)\n",
    "  # visitors_reponse = conn.getresponse().read().decode(\"utf-8\")\n",
    "  # visitors_json = json.loads(visitors_reponse)\n",
    "  # GETVisitorsList = []\n",
    "  # GETVisitorsHistoryList = []\n",
    "  # for item in visitors_json:\n",
    "  #   visitor_id = item['visitorId']\n",
    "  #   GETVisitorsList.append(\"/api/v1/visitor/\"+visitor_id)\n",
    "  #   GETVisitorsHistoryList.append(\"/api/v1/visitor/\"+visitor_id+\"/history?starttime=\"+AggregationData_StartDate)\n",
    "  # conn.close()\n",
    "\n",
    "\n",
    "\n",
    "  # Events payload aggregation = week\n",
    "  Events_payload = json.dumps({\n",
    "    \"response\": {\n",
    "      \"mimeType\": \"application/json\"\n",
    "    },\n",
    "    \"request\": {\n",
    "      \"pipeline\": [\n",
    "        {\n",
    "          \"source\": {\n",
    "            \"events\": None,\n",
    "            \"timeSeries\": {\n",
    "              \"period\": \"weekRange\",\n",
    "              \"first\": f\"{AggregationData_StartDate}\",\n",
    "              \"last\":f\"{AggregationData_EndDate}\"\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  })\n",
    "\n",
    "  # PageEvents payload aggregation = week\n",
    "  PageEvents_payload = json.dumps({\n",
    "    \"response\": {\n",
    "      \"mimeType\": \"application/json\"\n",
    "    },\n",
    "    \"request\": {\n",
    "      \"pipeline\": [\n",
    "        {\n",
    "          \"source\": {\n",
    "            \"pageEvents\": None,\n",
    "            \"timeSeries\": {\n",
    "              \"period\": \"weekRange\",\n",
    "              \"first\": f\"{AggregationData_StartDate}\",\n",
    "              \"last\":f\"{AggregationData_EndDate}\"\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  })\n",
    "\n",
    "  # FeatureEvents payload aggregation = week\n",
    "  FeatureEvents_payload = json.dumps({\n",
    "    \"response\": {\n",
    "      \"mimeType\": \"application/json\"\n",
    "    },\n",
    "    \"request\": {\n",
    "      \"pipeline\": [\n",
    "        {\n",
    "          \"source\": {\n",
    "            \"featureEvents\": None,\n",
    "            \"timeSeries\": {\n",
    "              \"period\": \"weekRange\",\n",
    "              \"first\": f\"{AggregationData_StartDate}\",\n",
    "              \"last\":f\"{AggregationData_EndDate}\"\n",
    "            }        \n",
    "          }\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  })\n",
    "\n",
    "  # guideEvents payload aggregation = week\n",
    "  GuideEvents_payload = json.dumps({\n",
    "    \"response\": {\n",
    "      \"mimeType\": \"application/json\"\n",
    "    },\n",
    "    \"request\": {\n",
    "      \"pipeline\": [\n",
    "        {\n",
    "          \"source\": {\n",
    "            \"guideEvents\": None,\n",
    "            \"timeSeries\": {\n",
    "              \"period\": \"weekRange\",\n",
    "              \"first\": f\"{AggregationData_StartDate}\",\n",
    "              \"last\":f\"{AggregationData_EndDate}\"\n",
    "            }        \n",
    "          }\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  })\n",
    "\n",
    "  # TrackEvents payload aggregation = week\n",
    "  TrackEvents_payload = json.dumps({\n",
    "    \"response\": {\n",
    "      \"mimeType\": \"application/json\"\n",
    "    },\n",
    "    \"request\": {\n",
    "      \"pipeline\": [\n",
    "        {\n",
    "          \"source\": {\n",
    "            \"trackEvents\": None,\n",
    "            \"timeSeries\": {\n",
    "              \"period\": \"weekRange\",\n",
    "              \"first\": f\"{AggregationData_StartDate}\",\n",
    "              \"last\":f\"{AggregationData_EndDate}\"\n",
    "            }        \n",
    "          }\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  })\n",
    "\n",
    "  # PollEvents payload aggregation = week\n",
    "  PollEvents_payload = json.dumps({\n",
    "    \"response\": {\n",
    "      \"mimeType\": \"application/json\"\n",
    "    },\n",
    "    \"request\": {\n",
    "      \"pipeline\": [\n",
    "        {\n",
    "          \"source\": {\n",
    "            \"pollEvents\": None,\n",
    "            \"timeSeries\": {\n",
    "              \"period\": \"weekRange\",\n",
    "              \"first\": f\"{AggregationData_StartDate}\",\n",
    "              \"last\":f\"{AggregationData_EndDate}\"\n",
    "            }        \n",
    "          }\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  })\n",
    "\n",
    "\n",
    "  # Features payload aggregation data\n",
    "  Features_payload = json.dumps({\n",
    "    \"response\": {\n",
    "      \"mimeType\": \"application/json\"\n",
    "    },\n",
    "    \"request\": {\n",
    "      \"name\": \"features source\",\n",
    "      \"pipeline\": [\n",
    "        {\n",
    "          \"source\": {\n",
    "            \"features\": None\n",
    "          }\n",
    "        },\n",
    "        {\n",
    "          \"filter\":\"createdAt>=\"+f\"{AggregationData_StartDate}\"+\" || lastUpdatedAt>=\"+f\"{AggregationData_StartDate}\"\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  })\n",
    "\n",
    "  # Pages payload aggregation data\n",
    "  Pages_payload = json.dumps({\n",
    "    \"response\": {\n",
    "      \"mimeType\": \"application/json\"\n",
    "    },\n",
    "    \"request\": {\n",
    "      \"name\": \"pages source\",\n",
    "      \"pipeline\": [\n",
    "        {\n",
    "          \"source\": {\n",
    "            \"pages\": None\n",
    "          }\n",
    "        },\n",
    "        {\n",
    "          \"filter\":\"createdAt>=\"+f\"{AggregationData_StartDate}\"+\" || lastUpdatedAt>=\"+f\"{AggregationData_StartDate}\"\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  })\n",
    "\n",
    "  # guides payload aggregation data\n",
    "  Guides_payload = json.dumps({\n",
    "    \"response\": {\n",
    "      \"mimeType\": \"application/json\"\n",
    "    },\n",
    "    \"request\": {\n",
    "      \"name\": \"guides source\",\n",
    "      \"pipeline\": [\n",
    "        {\n",
    "          \"source\": {\n",
    "            \"guides\": None\n",
    "          }\n",
    "        },\n",
    "        {\n",
    "          \"filter\":\"createdAt>=\"+f\"{AggregationData_StartDate}\"+\" || lastUpdatedAt>=\"+f\"{AggregationData_StartDate}\"\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  })\n",
    "\n",
    "  # Accounts payload aggregation data\n",
    "  Accounts_payload = json.dumps({\n",
    "    \"response\": {\n",
    "      \"mimeType\": \"application/json\"\n",
    "    },\n",
    "    \"request\": {\n",
    "      \"name\": \"accounts source\",\n",
    "      \"pipeline\": [\n",
    "        {\n",
    "          \"source\": {\n",
    "            \"accounts\": None \n",
    "          }\n",
    "        },\n",
    "        {\n",
    "          \"filter\":\"metadata.auto.firstvisit>=\"+f\"{AggregationData_StartDate}\" + \" || metadata.auto.lastUpdated>=\"+f\"{AggregationData_StartDate}\"\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  })\n",
    "\n",
    "  # Visitors payload aggregation data\n",
    "  Visitors_payload = json.dumps({\n",
    "    \"response\": {\n",
    "        \"mimeType\": \"application/json\"\n",
    "    },\n",
    "    \"request\": {\n",
    "        \"name\": \"visitors source\",\n",
    "        \"pipeline\": [\n",
    "            {\n",
    "              \"source\": {\n",
    "                  \"visitors\": None\n",
    "              }\n",
    "            },\n",
    "            {\n",
    "              \"filter\":\"metadata.auto.firstvisit>=\"+f\"{AggregationData_StartDate}\" + \" || metadata.auto.lastUpdated>=\"+f\"{AggregationData_StartDate}\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "  })\n",
    "\n",
    "\n",
    "\n",
    "  GETFeatures = ['/api/v1/feature?expand=*']\n",
    "  GETPages = ['/api/v1/page?expand=*']\n",
    "  GETGuides = ['/api/v1/guide?expand=*']\n",
    "  GETMetadataVisitor = ['/api/v1/metadata/schema/visitor']\n",
    "  GETMetadataAccount = ['/api/v1/metadata/schema/account']\n",
    "  GETAggregation = ['/api/v1/aggregation']\n",
    "\n",
    "\n",
    "  # get_pendo_RawData('VisitorMetadata',GETMetadataVisitor)\n",
    "  # get_pendo_RawData('AcconutMetadata',GETMetadataAccount)\n",
    "  # get_pendo_RawData('PendoRaw_VisitorsHistory', GETVisitorsHistoryList[:3])\n",
    "  # get_pendo_RawData('Dependencies', GETMetadataDependencies)\n",
    "  # get_pendo_RawData('TrackTypes','/api/v1/tracktype') # No Track Events data for Aspire\n",
    "  # get_pendo_RawData('Accounts',GETAccountsList)\n",
    "  # get_pendo_RawData('Visitors',GETVisitorsList)\n",
    "\n",
    "\n",
    "  get_pendo_RawData('Features',GETFeatures)\n",
    "  get_pendo_RawData('Pages',GETPages)\n",
    "  get_pendo_RawData('Guides',GETGuides)\n",
    "\n",
    "  get_pendo_RawDataAggregation('FeaturesAggregation',Features_payload, Data_WriteMode, GETAggregation)\n",
    "  get_pendo_RawDataAggregation('PagesAggregation',Pages_payload, Data_WriteMode, GETAggregation)\n",
    "  get_pendo_RawDataAggregation('GuidesAggregation',Guides_payload, Data_WriteMode, GETAggregation)\n",
    "  get_pendo_RawDataAggregation('AccountsAggregation',Accounts_payload, Data_WriteMode, GETAggregation)\n",
    "  get_pendo_RawDataAggregation('VisitorsAggregation',Visitors_payload, Data_WriteMode, GETAggregation)\n",
    "\n",
    "  get_pendo_RawDataAggregation('EventsFinal',Events_payload, Data_WriteMode, GETAggregation) \n",
    "  get_pendo_RawDataAggregation('PageEventsFinal',PageEvents_payload, Data_WriteMode, GETAggregation)\n",
    "  get_pendo_RawDataAggregation('FeatureEventsFinal',FeatureEvents_payload, Data_WriteMode, GETAggregation)\n",
    "  get_pendo_RawDataAggregation('GuideEventsFinal',GuideEvents_payload, Data_WriteMode, GETAggregation)\n",
    "  get_pendo_RawDataAggregation('TrackEventsFinal',TrackEvents_payload, Data_WriteMode, GETAggregation)\n",
    "  get_pendo_RawDataAggregation('PollEventsFinal',PollEvents_payload, Data_WriteMode, GETAggregation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d0b253b-9434-4ec0-9683-c6cba59b979e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "import time \n",
    "\n",
    "Data_WriteMode = \"append\" # overwrite\n",
    "\n",
    "# Correctly specifying the catalog and database\n",
    "spark.sql(\"USE CATALOG silver_prod_fr\")\n",
    "\n",
    "# Create metadata schema\n",
    "metadata_schema = \"\"\"\n",
    "    object_name STRING,\n",
    "    record_timestamp DATE\n",
    "\"\"\"\n",
    "\n",
    "# Create the metadata table if it doesn't exist\n",
    "spark.sql(f\"CREATE TABLE IF NOT EXISTS pendo.events_metadata ({metadata_schema}) USING DELTA\")\n",
    "\n",
    "def is_data_loaded(object_name, record_timestamp):\n",
    "    # Check if data with given timestamp is already loaded.\n",
    "    # Connect to the metadata table and check if data exists for the given object and timestamp\n",
    "    result = spark.sql(f\"\"\"\n",
    "        SELECT COUNT(*)\n",
    "        FROM pendo.events_metadata\n",
    "        WHERE object_name = '{object_name}'\n",
    "        AND record_timestamp = '{record_timestamp}'\n",
    "    \"\"\").collect()\n",
    "\n",
    "    # If count > 0, data already loaded\n",
    "    return result[0][0] > 0\n",
    "  \n",
    "def insert_into_metadata(object_name, record_timestamp):\n",
    "    # Insert a record into the metadata table.\n",
    "    spark.conf.set(\"fs.azure.account.key.greenindustrydeltalake.dfs.core.windows.net\", storage_account_key)\n",
    "    metadata_df = spark.createDataFrame([Row(object_name=object_name, record_timestamp=record_timestamp)], schema=metadata_schema)\n",
    "    metadata_df.write.format(\"delta\").mode(\"append\").option(\"mergeSchema\",\"true\").saveAsTable(\"pendo.events_metadata\")\n",
    "\n",
    "current_time_seconds = time.time()\n",
    "current_time_ms = int(current_time_seconds * 1000)\n",
    "for day in range(821): \n",
    "    GETAggregation = ['/api/v1/aggregation']\n",
    "    start_of_day_ms = current_time_ms - (day * 24 * 60 * 60 * 1000)\n",
    "    start_of_day_date = datetime.datetime.fromtimestamp(start_of_day_ms / 1000).date()\n",
    "\n",
    "    if not is_data_loaded('Events', start_of_day_date):\n",
    "        Events_payload = json.dumps({\n",
    "        \"response\": {\n",
    "            \"mimeType\": \"application/json\"\n",
    "        },\n",
    "        \"request\": {\n",
    "            \"pipeline\": [\n",
    "            {\n",
    "                \"source\": {\n",
    "                \"events\": None,\n",
    "                \"timeSeries\": \n",
    "                    {\n",
    "                    \"period\": \"dayRange\",\n",
    "                    \"first\": start_of_day_ms,\n",
    "                    \"count\": 1\n",
    "                }\n",
    "                }\n",
    "            }\n",
    "            ]\n",
    "        }\n",
    "        })\n",
    "        get_pendo_RawDataAggregation('Events2',Events_payload, Data_WriteMode, GETAggregation) \n",
    "        insert_into_metadata('Events', start_of_day_date)\n",
    "        print(\"Events data loaded for day:\", day)\n",
    "    \n",
    "\n",
    "    elif not is_data_loaded('PageEvents', start_of_day_date):\n",
    "        PageEvents_payload = json.dumps({\n",
    "        \"response\": {\n",
    "            \"mimeType\": \"application/json\"\n",
    "        },\n",
    "        \"request\": {\n",
    "            \"pipeline\": [\n",
    "            {\n",
    "                \"source\": {\n",
    "                \"pageEvents\": None,\n",
    "                \"timeSeries\": \n",
    "                    {\n",
    "                    \"period\": \"dayRange\",\n",
    "                    \"first\": start_of_day_ms,\n",
    "                    \"count\": 1\n",
    "                }\n",
    "                }\n",
    "            }\n",
    "            ]\n",
    "        }\n",
    "        })\n",
    "        get_pendo_RawDataAggregation('PageEvents2',PageEvents_payload, Data_WriteMode, GETAggregation)\n",
    "        insert_into_metadata('PageEvents', start_of_day_date)\n",
    "        print(\"PageEvents data loaded for day:\", day)\n",
    "\n",
    "    elif not is_data_loaded('FeatureEvents', start_of_day_date):\n",
    "        FeatureEvents_payload = json.dumps({\n",
    "        \"response\": {\n",
    "                \"mimeType\": \"application/json\"\n",
    "            },\n",
    "            \"request\": {\n",
    "                \"pipeline\": [\n",
    "                    {\n",
    "                        \"source\": {\n",
    "                            \"featureEvents\": None,\n",
    "                            \"timeSeries\": \n",
    "                            {\n",
    "                            \"period\": \"dayRange\",\n",
    "                            \"first\": start_of_day_ms,\n",
    "                            \"count\": 1\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        })\n",
    "        get_pendo_RawDataAggregation('FeatureEvents2',FeatureEvents_payload, Data_WriteMode, GETAggregation)\n",
    "        insert_into_metadata('FeatureEvents', start_of_day_date)\n",
    "        print(\"FeatureEvents data loaded for day:\", day)\n",
    "    \n",
    "    elif not is_data_loaded('TrackEvents', start_of_day_date):\n",
    "        TrackEvents_payload = json.dumps({\n",
    "        \"response\": {\n",
    "                \"mimeType\": \"application/json\"\n",
    "            },\n",
    "            \"request\": {\n",
    "                \"pipeline\": [\n",
    "                    {\n",
    "                        \"source\": {\n",
    "                            \"trackEvents\": None,\n",
    "                            \"timeSeries\": \n",
    "                            {\n",
    "                            \"period\": \"dayRange\",\n",
    "                            \"first\": start_of_day_ms,\n",
    "                                \"count\": 1\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        })\n",
    "        get_pendo_RawDataAggregation('TrackEvents2',TrackEvents_payload, Data_WriteMode, GETAggregation)\n",
    "        insert_into_metadata('TrackEvents', start_of_day_date)\n",
    "        print(\"TrackEvents data loaded for day:\", day)\n",
    "\n",
    "    elif not is_data_loaded('PollEvents', start_of_day_date):\n",
    "        PollEvents_payload = json.dumps({\n",
    "        \"response\": {\n",
    "                \"mimeType\": \"application/json\"\n",
    "            },\n",
    "            \"request\": {\n",
    "                \"pipeline\": [\n",
    "                    {\n",
    "                        \"source\": {\n",
    "                            \"pollEvents\": None,\n",
    "                            \"timeSeries\": \n",
    "                            {\n",
    "                            \"period\": \"dayRange\",\n",
    "                            \"first\": start_of_day_ms,\n",
    "                            \"count\": 1\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        })\n",
    "        get_pendo_RawDataAggregation('PollEvents2',PollEvents_payload, Data_WriteMode, GETAggregation)\n",
    "        insert_into_metadata('PollEvents', start_of_day_date)\n",
    "        print(\"PollEvents data loaded for day:\", day)\n",
    "\n",
    "    elif not is_data_loaded('GuideEvents', start_of_day_date):\n",
    "        GuideEvents_payload = json.dumps({\n",
    "        \"response\": {\n",
    "                \"mimeType\": \"application/json\"\n",
    "            },\n",
    "            \"request\": {\n",
    "                \"pipeline\": [\n",
    "                    {\n",
    "                        \"source\": {\n",
    "                            \"guideEvents\": None,\n",
    "                            \"timeSeries\": \n",
    "                            {\n",
    "                            \"period\": \"dayRange\",\n",
    "                            \"first\": start_of_day_ms,\n",
    "                            \"count\": 1\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        })\n",
    "        get_pendo_RawDataAggregation('GuideEvents2',GuideEvents_payload, Data_WriteMode, GETAggregation)\n",
    "        insert_into_metadata('GuideEvents', start_of_day_date)\n",
    "        print(\"GuideEvents data loaded for day:\", day)\n",
    "    else:\n",
    "        print(\"All days have been loaded for all events\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "4f5da13a-593a-4aa4-a53b-ff47788956c1",
     "showTitle": true,
     "title": "Pendo Silver Layer calls"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Today_date = spark.sql(\"Select date_format(current_date(),'yyyyMMdd')\").collect()[0][0] \n",
    "\n",
    "# Visitors_data = spark.read.format(\"delta\").load(\"url_to_visitors_data_in_storage_account\")\n",
    "# # Show the content of the DataFrame\n",
    "# Visitors_data.show()  \n",
    "# Visitors_data.write.format(\"delta\").mode(\"append\").option(\"mergeSchema\",\"true\").saveAsTable(\"pendo_all_visitors_test\")\n",
    "# print(\"Visitors data read complete\")\n",
    "\n",
    "# Accounts_data = spark.read.format(\"delta\").load(\"url_to_accounts_data_in_storage_account\")\n",
    "# # Show the content of the DataFrame\n",
    "# Accounts_data.show()  \n",
    "# Accounts_data.write.format(\"delta\").mode(\"append\").option(\"mergeSchema\",\"true\").saveAsTable(\"pendo_all_accounts_test\")\n",
    "# print(\"Accounts data read complete\")\n",
    "\n",
    "# Pages_data = spark.read.format(\"delta\").load(\"url_to_pages_data_in_storage_account\")\n",
    "# # # Show the content of the DataFrame\n",
    "# # Pages_data.show()  \n",
    "# Pages_data.write.format(\"delta\").mode(\"append\").option(\"mergeSchema\",\"true\").saveAsTable(\"pendo_all_pages_data_test\")\n",
    "# print(\"Pages data read complete\")\n",
    "\n",
    "# Features_data = spark.read.format(\"delta\").load(\"url_to_features_data_in_storage_account\")\n",
    "# # # Show the content of the DataFrame\n",
    "# # Features_data.show()  \n",
    "# Features_data.write.format(\"delta\").mode(\"append\").option(\"mergeSchema\",\"true\").saveAsTable(\"pendo_all_features_test\")\n",
    "# print(\"Features data read complete\")\n",
    "\n",
    "# Dependencies_data = spark.read.format(\"delta\").load(\"url_to_dependencies_data_in_storage_account\")\n",
    "# # # Show the content of the DataFrame\n",
    "# # Dependencies_data.show()  \n",
    "# Dependencies_data.write.format(\"delta\").mode(\"append\").option(\"mergeSchema\",\"true\").saveAsTable(\"pendo_all_Dependencies_test\")\n",
    "# print(\"Dependencies data read complete\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Pendo FR - Bronze",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
