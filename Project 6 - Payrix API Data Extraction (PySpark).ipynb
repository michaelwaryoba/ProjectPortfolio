{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d4ff4bf-bb1e-45c4-ba04-a5fc04ec2d2f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Import required libraries\n",
    "import requests\n",
    "import json\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Configure Payrix API Details\n",
    "api_url = \"https://api.payrix.com/\"\n",
    "api_key = dbutils.secrets.get(\"aspire-analyticsprod\", \"fieldroutes-analytics-payrix-api\")\n",
    "# '7a58ec5f0ddd3d9361e36d9a6d0a454a'\n",
    "headers = {'APIKEY': api_key,'Accept': 'application/json'}\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Configure Storage Account Details\n",
    "storage_account_name = dbutils.secrets.get(\"aspire-analyticsprod\", \"greenindustrydeltalake-storagename\")\n",
    "container_name = 'fieldroutes'\n",
    "data_channel = \"payrix\"\n",
    "medallion_stage_destination = \"bronze\"\n",
    "storage_accountkey = dbutils.secrets.get(\"aspire-analyticsprod\", \"adls-greenindustrydeltalake-accesskey\")\n",
    "\n",
    "# Set ADLS Config\n",
    "spark.conf.set(f\"fs.azure.account.key.{storage_account_name}.dfs.core.windows.net\", f\"{storage_accountkey}\")\n",
    "\n",
    "# Set data destination adls path\n",
    "destination_path = f\"abfss://{container_name}@{storage_account_name}.dfs.core.windows.net/{data_channel}/{medallion_stage_destination}\"\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"Payrix Data Load\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "343522e5-fbf2-4073-8304-c6251102468d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def fetch_data_with_retry(url, headers, params, resource_name, retries=2, timeout=30):\n",
    "    # Added a timeout parameter which defaults to 30 seconds\n",
    "\n",
    "    for attempt in range(retries):\n",
    "\n",
    "        try:\n",
    "            # Added a timeout to the requests.get call\n",
    "            response = requests.get(url, headers=headers, params=params, timeout=timeout)\n",
    "            response.raise_for_status()\n",
    "            return response.json()\n",
    "        except requests.exceptions.Timeout:\n",
    "            # Catches timeout specific errors\n",
    "            print(f\"Attempt {attempt+1} timed out for {resource_name}\")\n",
    "        except requests.exceptions.HTTPError as e:\n",
    "            print(f\"Attempt {attempt+1} failed with status code {response.status_code} for {resource_name}: {str(e)}\")\n",
    "            if response.status_code == 401:  # Unauthorized\n",
    "                print(\"Invalid authentication, please check your API key.\")\n",
    "                break\n",
    "            if response.status_code == 400:  # Bad Request\n",
    "                print(\"Bad request, please check your request parameters.\")\n",
    "                break\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            # This will catch other exceptions such as a connection error\n",
    "            print(f\"Attempt {attempt+1} failed for {resource_name} due to a connection error: {str(e)}\")\n",
    "\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "486a94e7-5ed1-444e-96c4-f011021f2e4a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Pulling in data using a strt date of 01/01/2022\n",
    "\n",
    "def fetch_and_save_data(api_endpoint, resource_name, destination_path, params={}):\n",
    "    start_date = datetime(2024, 5, 13)\n",
    "    current_date = datetime.now()\n",
    "\n",
    "    print(f\"Data fetch for {resource_name} started.\")\n",
    "\n",
    "    data_list = []  # Initialize an empty list to store all data\n",
    "\n",
    "    while start_date <= current_date:\n",
    "        date_str = start_date.strftime('%Y-%m-%d')\n",
    "        params['search'] = f\"created[equals]={date_str}\"\n",
    "        print(f\"Fetching data from {date_str}\")  # Print the page number\n",
    "        data = fetch_data_with_retry(api_endpoint, headers, params, resource_name)\n",
    "        if not data or 'data' not in data['response'] or not data['response']['data']:\n",
    "            break  # No more data to fetch\n",
    "\n",
    "        # Convert each item in the list to a JSON string and add it to data_list\n",
    "        data_list.extend([json.dumps(item) for item in data['response']['data']])\n",
    "        # print(f\"Number of records in list after appending: {len(data_list)}\")  # Print the number of records in list\n",
    "\n",
    "        # Increment the day\n",
    "        start_date += timedelta(days=1)\n",
    "\n",
    "    # Define a schema with a single column named 'json_string'\n",
    "    schema = StructType([StructField(\"json_string\", StringType(), True)])\n",
    "\n",
    "    try:\n",
    "        # Convert the JSON strings into a DataFrame\n",
    "        df = spark.createDataFrame(data_list, StringType()).toDF(\"json_string\")\n",
    "\n",
    "        # Save the DataFrame as JSON files\n",
    "        df.write.format(\"delta\").mode(\"append\").save(f\"{destination_path}/{resource_name}\")\n",
    "\n",
    "        # If you want to check and print the number of rows written, perform an action that triggers a job, like count()\n",
    "        print(f\"Data for {resource_name} fetched and saved to {destination_path}/{resource_name}, rows count: {df.count()}.\\n\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing data for {resource_name}: {str(e)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d70895af-55fc-4b22-89fd-f4006624ef55",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data fetch for payouts started.\n",
      "Fetching data from 2024-05-13\n",
      "Fetching data from 2024-05-14\n",
      "Fetching data from 2024-05-15\n",
      "Fetching data from 2024-05-16\n",
      "Fetching data from 2024-05-17\n",
      "Fetching data from 2024-05-18\n",
      "Fetching data from 2024-05-19\n",
      "Fetching data from 2024-05-20\n",
      "Fetching data from 2024-05-21\n",
      "Fetching data from 2024-05-22\n",
      "Data for payouts fetched and saved to abfss://fieldroutes@[REDACTED].dfs.core.windows.net/payrix/bronze/payouts, rows count: 300.\n",
      "\n",
      "Data fetch for plans started.\n",
      "Fetching data from 2024-05-13\n",
      "Fetching data from 2024-05-14\n",
      "Fetching data from 2024-05-15\n",
      "Fetching data from 2024-05-16\n",
      "Fetching data from 2024-05-17\n",
      "Fetching data from 2024-05-18\n",
      "Fetching data from 2024-05-19\n",
      "Fetching data from 2024-05-20\n",
      "Fetching data from 2024-05-21\n",
      "Fetching data from 2024-05-22\n",
      "Data for plans fetched and saved to abfss://fieldroutes@[REDACTED].dfs.core.windows.net/payrix/bronze/plans, rows count: 160.\n",
      "\n",
      "Data fetch for subscriptions started.\n",
      "Fetching data from 2024-05-13\n",
      "Fetching data from 2024-05-14\n",
      "Fetching data from 2024-05-15\n",
      "Fetching data from 2024-05-16\n",
      "Fetching data from 2024-05-17\n",
      "Fetching data from 2024-05-18\n",
      "Fetching data from 2024-05-19\n",
      "Fetching data from 2024-05-20\n",
      "Fetching data from 2024-05-21\n",
      "Fetching data from 2024-05-22\n",
      "Data for subscriptions fetched and saved to abfss://fieldroutes@[REDACTED].dfs.core.windows.net/payrix/bronze/subscriptions, rows count: 40.\n",
      "\n",
      "Data fetch for teamLogins started.\n",
      "Fetching data from 2024-05-13\n",
      "Fetching data from 2024-05-14\n",
      "Fetching data from 2024-05-15\n",
      "Fetching data from 2024-05-16\n",
      "Fetching data from 2024-05-17\n",
      "Fetching data from 2024-05-18\n",
      "Fetching data from 2024-05-19\n",
      "Fetching data from 2024-05-20\n",
      "Fetching data from 2024-05-21\n",
      "Fetching data from 2024-05-22\n",
      "Data for teamLogins fetched and saved to abfss://fieldroutes@[REDACTED].dfs.core.windows.net/payrix/bronze/teamLogins, rows count: 300.\n",
      "\n",
      "Data fetch for txns started.\n",
      "Fetching data from 2024-05-13\n",
      "Fetching data from 2024-05-14\n",
      "Fetching data from 2024-05-15\n",
      "Fetching data from 2024-05-16\n",
      "Fetching data from 2024-05-17\n",
      "Fetching data from 2024-05-18\n",
      "Fetching data from 2024-05-19\n",
      "Fetching data from 2024-05-20\n",
      "Fetching data from 2024-05-21\n",
      "Fetching data from 2024-05-22\n",
      "Data for txns fetched and saved to abfss://fieldroutes@[REDACTED].dfs.core.windows.net/payrix/bronze/txns, rows count: 300.\n",
      "\n",
      "Data fetch for customers started.\n",
      "Fetching data from 2024-05-13\n",
      "Fetching data from 2024-05-14\n",
      "Fetching data from 2024-05-15\n",
      "Fetching data from 2024-05-16\n",
      "Fetching data from 2024-05-17\n",
      "Fetching data from 2024-05-18\n",
      "Fetching data from 2024-05-19\n",
      "Fetching data from 2024-05-20\n",
      "Fetching data from 2024-05-21\n",
      "Fetching data from 2024-05-22\n",
      "Data for customers fetched and saved to abfss://fieldroutes@[REDACTED].dfs.core.windows.net/payrix/bronze/customers, rows count: 300.\n",
      "\n",
      "Data fetch for invoices started.\n",
      "Fetching data from 2024-05-13\n",
      "Fetching data from 2024-05-14\n",
      "Fetching data from 2024-05-15\n",
      "Fetching data from 2024-05-16\n",
      "Fetching data from 2024-05-17\n",
      "Fetching data from 2024-05-18\n",
      "Fetching data from 2024-05-19\n",
      "Fetching data from 2024-05-20\n",
      "Fetching data from 2024-05-21\n",
      "Fetching data from 2024-05-22\n",
      "Data for invoices fetched and saved to abfss://fieldroutes@[REDACTED].dfs.core.windows.net/payrix/bronze/invoices, rows count: 240.\n",
      "\n",
      "Data fetch for billingEvents started.\n",
      "Fetching data from 2024-05-13\n",
      "Fetching data from 2024-05-14\n",
      "Fetching data from 2024-05-15\n",
      "Fetching data from 2024-05-16\n",
      "Fetching data from 2024-05-17\n",
      "Fetching data from 2024-05-18\n",
      "Fetching data from 2024-05-19\n",
      "Fetching data from 2024-05-20\n",
      "Fetching data from 2024-05-21\n",
      "Fetching data from 2024-05-22\n",
      "Data for billingEvents fetched and saved to abfss://fieldroutes@[REDACTED].dfs.core.windows.net/payrix/bronze/billingEvents, rows count: 110.\n",
      "\n",
      "Data fetch for billingModifiers started.\n",
      "Fetching data from 2024-05-13\n",
      "Data for billingModifiers fetched and saved to abfss://fieldroutes@[REDACTED].dfs.core.windows.net/payrix/bronze/billingModifiers, rows count: 0.\n",
      "\n",
      "Data fetch for billings started.\n",
      "Fetching data from 2024-05-13\n",
      "Fetching data from 2024-05-14\n",
      "Fetching data from 2024-05-15\n",
      "Fetching data from 2024-05-16\n",
      "Fetching data from 2024-05-17\n",
      "Fetching data from 2024-05-18\n",
      "Fetching data from 2024-05-19\n",
      "Fetching data from 2024-05-20\n",
      "Fetching data from 2024-05-21\n",
      "Fetching data from 2024-05-22\n",
      "Data for billings fetched and saved to abfss://fieldroutes@[REDACTED].dfs.core.windows.net/payrix/bronze/billings, rows count: 110.\n",
      "\n",
      "Data fetch for bins started.\n",
      "Fetching data from 2024-05-13\n",
      "Fetching data from 2024-05-14\n",
      "Fetching data from 2024-05-15\n",
      "Fetching data from 2024-05-16\n",
      "Fetching data from 2024-05-17\n",
      "Fetching data from 2024-05-18\n",
      "Fetching data from 2024-05-19\n",
      "Fetching data from 2024-05-20\n",
      "Fetching data from 2024-05-21\n",
      "Fetching data from 2024-05-22\n",
      "Data for bins fetched and saved to abfss://fieldroutes@[REDACTED].dfs.core.windows.net/payrix/bronze/bins, rows count: 60.\n",
      "\n",
      "Data fetch for changeRequests started.\n",
      "Fetching data from 2024-05-13\n",
      "Fetching data from 2024-05-14\n",
      "Fetching data from 2024-05-15\n",
      "Fetching data from 2024-05-16\n",
      "Fetching data from 2024-05-17\n",
      "Fetching data from 2024-05-18\n",
      "Fetching data from 2024-05-19\n",
      "Fetching data from 2024-05-20\n",
      "Fetching data from 2024-05-21\n",
      "Fetching data from 2024-05-22\n",
      "Data for changeRequests fetched and saved to abfss://fieldroutes@[REDACTED].dfs.core.windows.net/payrix/bronze/changeRequests, rows count: 300.\n",
      "\n",
      "Data fetch for chargebacks started.\n",
      "Fetching data from 2024-05-13\n",
      "Fetching data from 2024-05-14\n",
      "Fetching data from 2024-05-15\n",
      "Fetching data from 2024-05-16\n",
      "Fetching data from 2024-05-17\n",
      "Fetching data from 2024-05-18\n",
      "Fetching data from 2024-05-19\n",
      "Fetching data from 2024-05-20\n",
      "Fetching data from 2024-05-21\n",
      "Fetching data from 2024-05-22\n",
      "Data for chargebacks fetched and saved to abfss://fieldroutes@[REDACTED].dfs.core.windows.net/payrix/bronze/chargebacks, rows count: 300.\n",
      "\n",
      "Data fetch for contacts started.\n",
      "Fetching data from 2024-05-13\n",
      "Data for contacts fetched and saved to abfss://fieldroutes@[REDACTED].dfs.core.windows.net/payrix/bronze/contacts, rows count: 0.\n",
      "\n",
      "Data fetch for disbursements started.\n",
      "Fetching data from 2024-05-13\n",
      "Fetching data from 2024-05-14\n",
      "Fetching data from 2024-05-15\n",
      "Fetching data from 2024-05-16\n",
      "Fetching data from 2024-05-17\n",
      "Fetching data from 2024-05-18\n",
      "Fetching data from 2024-05-19\n",
      "Fetching data from 2024-05-20\n",
      "Fetching data from 2024-05-21\n",
      "Fetching data from 2024-05-22\n",
      "Data for disbursements fetched and saved to abfss://fieldroutes@[REDACTED].dfs.core.windows.net/payrix/bronze/disbursements, rows count: 300.\n",
      "\n",
      "Data fetch for divisions started.\n",
      "Fetching data from 2024-05-13\n",
      "Fetching data from 2024-05-14\n",
      "Fetching data from 2024-05-15\n",
      "Fetching data from 2024-05-16\n",
      "Fetching data from 2024-05-17\n",
      "Fetching data from 2024-05-18\n",
      "Fetching data from 2024-05-19\n",
      "Fetching data from 2024-05-20\n",
      "Fetching data from 2024-05-21\n",
      "Fetching data from 2024-05-22\n",
      "Data for divisions fetched and saved to abfss://fieldroutes@[REDACTED].dfs.core.windows.net/payrix/bronze/divisions, rows count: 10.\n",
      "\n",
      "Data fetch for holds started.\n",
      "Fetching data from 2024-05-13\n",
      "Fetching data from 2024-05-14\n",
      "Fetching data from 2024-05-15\n",
      "Fetching data from 2024-05-16\n",
      "Fetching data from 2024-05-17\n",
      "Fetching data from 2024-05-18\n",
      "Fetching data from 2024-05-19\n",
      "Fetching data from 2024-05-20\n",
      "Fetching data from 2024-05-21\n",
      "Fetching data from 2024-05-22\n",
      "Data for holds fetched and saved to abfss://fieldroutes@[REDACTED].dfs.core.windows.net/payrix/bronze/holds, rows count: 300.\n",
      "\n",
      "Data fetch for invoiceItems started.\n",
      "Fetching data from 2024-05-13\n",
      "Attempt 1 timed out for invoiceItems\n",
      "Attempt 2 timed out for invoiceItems\n",
      "Data for invoiceItems fetched and saved to abfss://fieldroutes@[REDACTED].dfs.core.windows.net/payrix/bronze/invoiceItems, rows count: 0.\n",
      "\n",
      "Data fetch for invoiceLineItems started.\n",
      "Fetching data from 2024-05-13\n",
      "Attempt 1 timed out for invoiceLineItems\n",
      "Attempt 2 timed out for invoiceLineItems\n",
      "Data for invoiceLineItems fetched and saved to abfss://fieldroutes@[REDACTED].dfs.core.windows.net/payrix/bronze/invoiceLineItems, rows count: 0.\n",
      "\n",
      "Data fetch for refunds started.\n",
      "Fetching data from 2024-05-13\n",
      "Fetching data from 2024-05-14\n",
      "Fetching data from 2024-05-15\n",
      "Fetching data from 2024-05-16\n",
      "Fetching data from 2024-05-17\n",
      "Fetching data from 2024-05-18\n",
      "Fetching data from 2024-05-19\n",
      "Fetching data from 2024-05-20\n",
      "Fetching data from 2024-05-21\n",
      "Fetching data from 2024-05-22\n",
      "Data for refunds fetched and saved to abfss://fieldroutes@[REDACTED].dfs.core.windows.net/payrix/bronze/refunds, rows count: 300.\n",
      "\n",
      "Data fetch for tokens started.\n",
      "Fetching data from 2024-05-13\n",
      "Fetching data from 2024-05-14\n",
      "Fetching data from 2024-05-15\n",
      "Fetching data from 2024-05-16\n",
      "Fetching data from 2024-05-17\n",
      "Fetching data from 2024-05-18\n",
      "Fetching data from 2024-05-19\n",
      "Fetching data from 2024-05-20\n",
      "Fetching data from 2024-05-21\n",
      "Fetching data from 2024-05-22\n",
      "Data for tokens fetched and saved to abfss://fieldroutes@[REDACTED].dfs.core.windows.net/payrix/bronze/tokens, rows count: 300.\n",
      "\n",
      "Data fetch for subscriptionTokens started.\n",
      "Fetching data from 2024-05-13\n",
      "Fetching data from 2024-05-14\n",
      "Fetching data from 2024-05-15\n",
      "Fetching data from 2024-05-16\n",
      "Fetching data from 2024-05-17\n",
      "Fetching data from 2024-05-18\n",
      "Fetching data from 2024-05-19\n",
      "Fetching data from 2024-05-20\n",
      "Fetching data from 2024-05-21\n",
      "Fetching data from 2024-05-22\n",
      "Data for subscriptionTokens fetched and saved to abfss://fieldroutes@[REDACTED].dfs.core.windows.net/payrix/bronze/subscriptionTokens, rows count: 40.\n",
      "\n",
      "Data fetch for mappings started.\n",
      "Fetching data from 2024-05-13\n",
      "Data for mappings fetched and saved to abfss://fieldroutes@[REDACTED].dfs.core.windows.net/payrix/bronze/mappings, rows count: 0.\n",
      "\n",
      "Data fetch for logins started.\n",
      "Fetching data from 2024-05-13\n",
      "Fetching data from 2024-05-14\n",
      "Fetching data from 2024-05-15\n",
      "Fetching data from 2024-05-16\n",
      "Fetching data from 2024-05-17\n",
      "Fetching data from 2024-05-18\n",
      "Fetching data from 2024-05-19\n",
      "Fetching data from 2024-05-20\n",
      "Fetching data from 2024-05-21\n",
      "Fetching data from 2024-05-22\n",
      "Data for logins fetched and saved to abfss://fieldroutes@[REDACTED].dfs.core.windows.net/payrix/bronze/logins, rows count: 300.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # List of endpoints to iterate over\n",
    "# endpoints = ['accounts', 'entities', 'members', 'funds', 'merchants', 'orgs', 'payouts', 'plans', 'subscriptions', 'teamLogins', 'txns', 'customers', 'invoices','billingEvents', 'billingModifiers', 'billings', 'bins', 'changeRequests', 'chargebacks', 'contacts', 'disbursements', 'divisions', 'holds', 'invoiceItems', 'invoiceLineItems', 'refunds','tokens','subscriptionTokens','mappings','logins']\n",
    "#error - refunds\n",
    "\n",
    "resource_names = ['accounts', 'entities', 'members', 'funds', 'merchants', 'orgs', 'payouts', 'plans', 'subscriptions', 'teamLogins', 'txns', 'customers', 'invoices','billingEvents', 'billingModifiers', 'billings', 'bins', 'changeRequests', 'chargebacks', 'contacts', 'disbursements', 'divisions', 'holds', 'invoiceItems', 'invoiceLineItems', 'refunds','tokens','subscriptionTokens','mappings','logins']\n",
    "\n",
    "for resource_name in resource_names:\n",
    "    api_endpoint = f\"{api_url}{resource_name}\"\n",
    "    fetch_and_save_data(api_endpoint, resource_name, destination_path)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "4. Payrix FR - Raw Data Bronze",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
