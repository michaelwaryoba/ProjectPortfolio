{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d4ff4bf-bb1e-45c4-ba04-a5fc04ec2d2f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Import required libraries\n",
    "import requests\n",
    "import json\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Configure Payrix API Details\n",
    "api_url = \"https://api.payrix.com/\"\n",
    "api_key = dbutils.secrets.get(\"fake-analyticsprod\", \"fake-api-key\")\n",
    "headers = {'APIKEY': api_key,'Accept': 'application/json'}\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Configure Storage Account Details\n",
    "storage_account_name = dbutils.secrets.get(\"fake-analyticsprod\", \"fake-storagename\")\n",
    "container_name = 'container_name'\n",
    "data_channel = \"data_channel\"\n",
    "medallion_stage_destination = \"medallion_layer\"\n",
    "storage_accountkey = dbutils.secrets.get(\"fake-analyticsprod\", \"fake-accesskey\")\n",
    "\n",
    "# Set ADLS Config\n",
    "spark.conf.set(f\"fs.azure.account.key.{storage_account_name}.dfs.core.windows.net\", f\"{storage_accountkey}\")\n",
    "\n",
    "# Set data destination adls path\n",
    "destination_path = f\"abfss://{container_name}@{storage_account_name}.dfs.core.windows.net/{data_channel}/{medallion_stage_destination}\"\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"Payrix Data Load\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "343522e5-fbf2-4073-8304-c6251102468d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def fetch_data_with_retry(url, headers, params, resource_name, retries=2, timeout=30):\n",
    "    # Added a timeout parameter which defaults to 30 seconds\n",
    "\n",
    "    for attempt in range(retries):\n",
    "\n",
    "        try:\n",
    "            # Added a timeout to the requests.get call\n",
    "            response = requests.get(url, headers=headers, params=params, timeout=timeout)\n",
    "            response.raise_for_status()\n",
    "            return response.json()\n",
    "        except requests.exceptions.Timeout:\n",
    "            # Catches timeout specific errors\n",
    "            print(f\"Attempt {attempt+1} timed out for {resource_name}\")\n",
    "        except requests.exceptions.HTTPError as e:\n",
    "            print(f\"Attempt {attempt+1} failed with status code {response.status_code} for {resource_name}: {str(e)}\")\n",
    "            if response.status_code == 401:  # Unauthorized\n",
    "                print(\"Invalid authentication, please check your API key.\")\n",
    "                break\n",
    "            if response.status_code == 400:  # Bad Request\n",
    "                print(\"Bad request, please check your request parameters.\")\n",
    "                break\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            # This will catch other exceptions such as a connection error\n",
    "            print(f\"Attempt {attempt+1} failed for {resource_name} due to a connection error: {str(e)}\")\n",
    "\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "486a94e7-5ed1-444e-96c4-f011021f2e4a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Pulling in data using a strt date of 01/01/2022\n",
    "\n",
    "def fetch_and_save_data(api_endpoint, resource_name, destination_path, params={}):\n",
    "    start_date = datetime(2022, 1, 1)\n",
    "    current_date = datetime.now()\n",
    "\n",
    "    print(f\"Data fetch for {resource_name} started.\")\n",
    "\n",
    "    data_list = []  # Initialize an empty list to store all data\n",
    "\n",
    "    while start_date <= current_date:\n",
    "        date_str = start_date.strftime('%Y-%m-%d')\n",
    "        params['search'] = f\"created[equals]={date_str}\"\n",
    "        print(f\"Fetching data from {date_str}\")  # Print the page number\n",
    "        data = fetch_data_with_retry(api_endpoint, headers, params, resource_name)\n",
    "        if not data or 'data' not in data['response'] or not data['response']['data']:\n",
    "            break  # No more data to fetch\n",
    "\n",
    "        # Convert each item in the list to a JSON string and add it to data_list\n",
    "        data_list.extend([json.dumps(item) for item in data['response']['data']])\n",
    "        # print(f\"Number of records in list after appending: {len(data_list)}\")  # Print the number of records in list\n",
    "\n",
    "        # Increment the day\n",
    "        start_date += timedelta(days=1)\n",
    "\n",
    "    # Define a schema with a single column named 'json_string'\n",
    "    schema = StructType([StructField(\"json_string\", StringType(), True)])\n",
    "\n",
    "    try:\n",
    "        # Convert the JSON strings into a DataFrame\n",
    "        df = spark.createDataFrame(data_list, StringType()).toDF(\"json_string\")\n",
    "\n",
    "        # Save the DataFrame as JSON files\n",
    "        df.write.format(\"delta\").mode(\"append\").save(f\"{destination_path}/{resource_name}\")\n",
    "\n",
    "        # If you want to check and print the number of rows written, perform an action that triggers a job, like count()\n",
    "        print(f\"Data for {resource_name} fetched and saved to {destination_path}/{resource_name}, rows count: {df.count()}.\\n\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing data for {resource_name}: {str(e)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d70895af-55fc-4b22-89fd-f4006624ef55",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # List of endpoints to iterate over\n",
    "# endpoints = ['accounts', 'entities', 'members', 'funds', 'merchants', 'orgs', 'payouts', 'plans', 'subscriptions', 'teamLogins', 'txns', 'customers', 'invoices','billingEvents', 'billingModifiers', 'billings', 'bins', 'changeRequests', 'chargebacks', 'contacts', 'disbursements', 'divisions', 'holds', 'invoiceItems', 'invoiceLineItems', 'refunds','tokens','subscriptionTokens','mappings','logins']\n",
    "#error - refunds\n",
    "\n",
    "resource_names = ['accounts', 'entities', 'members', 'funds', 'merchants', 'orgs', 'payouts', 'plans', 'subscriptions', 'teamLogins', 'txns', 'customers', 'invoices','billingEvents', 'billingModifiers', 'billings', 'bins', 'changeRequests', 'chargebacks', 'contacts', 'disbursements', 'divisions', 'holds', 'invoiceItems', 'invoiceLineItems', 'refunds','tokens','subscriptionTokens','mappings','logins']\n",
    "\n",
    "for resource_name in resource_names:\n",
    "    api_endpoint = f\"{api_url}{resource_name}\"\n",
    "    fetch_and_save_data(api_endpoint, resource_name, destination_path)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "4. Payrix FR - Raw Data Bronze",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
